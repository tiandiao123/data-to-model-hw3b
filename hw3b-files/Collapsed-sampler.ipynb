{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "from random import randint\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "alpha=0.1\n",
    "beta=0.01\n",
    "lmd=0.5\n",
    "num_of_iterations=10\n",
    "burnin=8\n",
    "outputfile=\"output.txt\"\n",
    "inputfile_test=\"input-test.txt\"\n",
    "inputfile_train=\"input-train.txt\"\n",
    "K=10#the number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=None\n",
    "with open(inputfile_train,'r') as f:\n",
    "\tdata=f.readlines()\n",
    "\tf.close()\n",
    "train_data=[]\n",
    "for line in data:\n",
    "\ttokens=line.strip(\"\\n\").split(\" \")\n",
    "\ttrain_data.append(tokens)\n",
    "\n",
    "\n",
    "test_data1=None\n",
    "with open(inputfile_test,'r') as f:\n",
    "\ttest_data1=f.readlines()\n",
    "\tf.close()\n",
    "test_data=[]\n",
    "for line in test_data1:\n",
    "\ttokens=line.strip(\"\\n\").split(\" \")\n",
    "\ttest_data.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocabulary_train=set()\n",
    "vocabulary_test=set()\n",
    "vocabulary=set()\n",
    "for tokens in train_data:\n",
    "    for token in tokens:\n",
    "        vocabulary.add(token)\n",
    "        vocabulary_train.add(token)\n",
    "V_train=len(vocabulary)\n",
    "\n",
    "for tokens in test_data:\n",
    "    for token in tokens:\n",
    "        vocabulary_test.add(token)\n",
    "        vocabulary.add(token)\n",
    "        \n",
    "list1=list(vocabulary)\n",
    "token2index={}\n",
    "for i in range(len(list1)):\n",
    "    token2index[list1[i]]=i\n",
    "\n",
    "V_test=len(vocabulary_test)\n",
    "V=len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8635"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5168"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8096"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_phi=[]\n",
    "for i in range(V):\n",
    "    global_phi.append([])\n",
    "    for j in range(K):\n",
    "        global_phi[i].append(0)\n",
    "        \n",
    "\n",
    "\n",
    "## create collection phi\n",
    "collection_phi=[]\n",
    "collection_phi.append([])\n",
    "collection_phi.append([])\n",
    "\n",
    "\n",
    "#collection_0_ph=[]\n",
    "for i in range(V):\n",
    "    collection_phi[0].append([])\n",
    "    collection_phi[1].append([])\n",
    "    for j in range(K):\n",
    "        collection_phi[0][i].append(0)\n",
    "        collection_phi[1][i].append(0)\n",
    "\n",
    "doc_topics=[]\n",
    "\n",
    "array=[]\n",
    "x=[]\n",
    "global_NumOftopics=[]\n",
    "for i in range(K):\n",
    "\tglobal_NumOftopics.append(0)\n",
    "\n",
    "c_NumOftopics=[]\n",
    "c_NumOftopics.append([])\n",
    "c_NumOftopics.append([])\n",
    "for i in range(K):\n",
    "\tc_NumOftopics[0].append(0)\n",
    "\tc_NumOftopics[1].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tokens in train_data:\n",
    "    #tokens=line.strip(\"\\n\").split(\" \")\n",
    "    array.append([])\n",
    "    doc_topics.append([])\n",
    "    x.append([])\n",
    "    size_of_doc=len(doc_topics)\n",
    "    for i in range(K):\n",
    "        doc_topics[size_of_doc-1].append(0)\n",
    "    for token in tokens:\n",
    "        #vocabulary.add(token)\n",
    "        random_topic=randint(0,K-1)\n",
    "        x_d_i=np.random.choice(2,1,p=[1-lmd,lmd])[0]\n",
    "        x[len(x)-1].append(x_d_i)\n",
    "        array[len(array)-1].append(random_topic)\n",
    "        global_NumOftopics[random_topic]+=1\n",
    "        # construct docs\n",
    "        doc_topics[size_of_doc-1][random_topic]+=1\n",
    "        global_phi[token2index[token]][random_topic]+=1\n",
    "        \n",
    "        c_NumOftopics[int(tokens[0])][random_topic]+=1\n",
    "        collection_phi[int(tokens[0])][token2index[token]][random_topic]+=1\n",
    "#             if tokens[0]=='1':\n",
    "#             \tc_NumOftopics[1][random_topic]+=1\n",
    "#             \tif token in collection_phi[1][random_topic]:\n",
    "#             \t\tcollection_phi[1][random_topic][token]+=1\n",
    "#             \telse:\n",
    "#             \t\tcollection_phi[1][random_topic][token]=1\n",
    "#             elif tokens[0]=='0':\n",
    "#             \tc_NumOftopics[0][random_topic]+=1\n",
    "#             \tif token in collection_phi[0][random_topic]:\n",
    "#             \t\tcollection_phi[0][random_topic][token]+=1\n",
    "#             \telse:\n",
    "#             \t\tcollection_phi[0][random_topic][token]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12115, 12111, 12187, 12146, 12313, 12235, 12057, 12182, 12222, 12111]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_NumOftopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test=[]\n",
    "array_test=[]\n",
    "doc_topics_test=[]\n",
    "global_NumOftopics_test=[]\n",
    "for i in range(K):\n",
    "\tglobal_NumOftopics_test.append(0)\n",
    "\n",
    "c_NumOftopics_test=[]\n",
    "c_NumOftopics_test.append([])\n",
    "c_NumOftopics_test.append([])\n",
    "for i in range(K):\n",
    "\tc_NumOftopics_test[0].append(0)\n",
    "\tc_NumOftopics_test[1].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=-1\n",
    "for tokens in test_data:\n",
    "    d+=1\n",
    "    #tokens=line.strip(\"\\n\").split(\" \")\n",
    "    array_test.append([])\n",
    "    x_test.append([])\n",
    "    doc_topics_test.append([])\n",
    "    for i in range(K):\n",
    "        doc_topics_test[len(doc_topics_test)-1].append(0)\n",
    "        \n",
    "    for i in range(len(tokens)):\n",
    "        random_topic=randint(0,K-1)\n",
    "        x_d_i=np.random.choice(2,1,p=[1-lmd,lmd])[0]\n",
    "\n",
    "        global_NumOftopics_test[random_topic]+=1\n",
    "        c_NumOftopics_test[int(tokens[0])][random_topic]+=1\n",
    "\n",
    "        doc_topics_test[len(doc_topics_test)-1][random_topic]+=1\n",
    "        x_test[len(x_test)-1].append(x_d_i)\n",
    "        array_test[d].append(random_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_distribution_doc_topic(flag,c_index,doc_index,token,doc_topics):\n",
    "    global K\n",
    "    global alpha\n",
    "    global beta\n",
    "    global vocabulary\n",
    "    global global_phi\n",
    "    global collection_phi\n",
    "    \n",
    "    distributions=[]\n",
    "    if flag==1:\n",
    "        for i in range(K):\n",
    "            distribution_i=(doc_topics[doc_index][i]+alpha)/(len(array[doc_index])-1+K*alpha)\n",
    "            sum_nk=c_NumOftopics[c_index][i]\n",
    "            distribution_i*=(collection_phi[c_index][token2index[token]][i]+beta)/(sum_nk+V_train*beta)\n",
    "#             if token in collection_phi[c_index][i]:\n",
    "#             \tdistribution_i*=(collection_phi[c_index][i][token]+beta)/(sum_nk+V*beta)\n",
    "#             else:\n",
    "#             \tdistribution_i*=(beta)/(sum_nk+V*beta)\n",
    "            distributions.append(distribution_i)\n",
    "    elif flag==0:\n",
    "        for i in range(K):\n",
    "            distribution_i=(doc_topics[doc_index][i]+alpha)/(len(array[doc_index])-1+K*alpha)\n",
    "            sum_nk=global_NumOftopics[i]\n",
    "            distribution_i*=(global_phi[token2index[token]][i]+beta)/(sum_nk+V_train*beta)\n",
    "#             if token in global_phi[i]:\n",
    "#             \tdistribution_i*=(global_phi[i][token]+beta)/(sum_nk+V*beta)\n",
    "#             else:\n",
    "#             \tdistribution_i*=beta/(sum_nk+V*beta)\n",
    "            distributions.append(distribution_i)\n",
    "    \n",
    "    return np.array(distributions)/np.sum(distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smaple_xdi(c_index,doc_index,token,x,z_d_token):\n",
    "    global vocabulary\n",
    "    global global_phi\n",
    "    global collection_phi\n",
    "    global lmd\n",
    "    global V\n",
    "    \n",
    "    sum_nk=global_NumOftopics[z_d_token]\n",
    "    p_0=(1-lmd)*((global_phi[token2index[token]][z_d_token]+beta)/(sum_nk+V_train*beta))\n",
    "#     if token in global_phi[z_d_token]:\n",
    "#     \tp_0=(1-lmd)*((global_phi[z_d_token][token]+beta)/(sum_nk+V*beta))\n",
    "#     else:\n",
    "#     \tp_0=(1-lmd)*(beta/(sum_nk+V*beta))\n",
    "\n",
    "    sum_nk_c=c_NumOftopics[c_index][z_d_token]\n",
    "    p_1=lmd*((collection_phi[c_index][token2index[token]][z_d_token]+beta)/(sum_nk_c+V_train*beta))\n",
    "#     if token in collection_phi[c_index][z_d_token]:\n",
    "#     \tp_1=lmd*((collection_phi[c_index][z_d_token][token]+beta)/(sum_nk_c+V*beta))\n",
    "#     else:\n",
    "#     \tp_1=lmd*(beta/(sum_nk_c+V*beta))\n",
    "    \n",
    "    dis= np.array([p_0 / (p_0+p_1), p_1 / (p_0+p_1)])#np.array(p)/np.sum(p)\n",
    "    return np.random.choice(2,1,p=dis)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_dic(z_d_token,token,token_index,doc_index,c_index,flag):\n",
    "    global vocabulary\n",
    "    global global_phi\n",
    "    global collection_phi\n",
    "    global doc_topics\n",
    "    global array\n",
    "    \n",
    "    original_topic=array[doc_index][token_index]\n",
    "    global_phi[token2index[token]][original_topic]-=1\n",
    "    global_phi[token2index[token]][z_d_token]+=1\n",
    "#     if token in global_phi[z_d_token]:\n",
    "#     \tglobal_phi[z_d_token][token]+=1\n",
    "#     else:\n",
    "#     \tglobal_phi[z_d_token][token]=1\n",
    "    collection_phi[c_index][token2index[token]][original_topic]-=1\n",
    "    collection_phi[c_index][token2index[token]][z_d_token]+=1\n",
    "#     if token in collection_phi[c_index][z_d_token]:\n",
    "#     \tcollection_phi[c_index][z_d_token][token]+=1\n",
    "#     else:\n",
    "#     \tcollection_phi[c_index][z_d_token][token]=1\n",
    "    array[doc_index][token_index]=z_d_token\n",
    "    doc_topics[doc_index][original_topic]-=1\n",
    "    doc_topics[doc_index][z_d_token]+=1\n",
    "    \n",
    "    global_NumOftopics[original_topic]-=1\n",
    "    global_NumOftopics[z_d_token]+=1\n",
    "    c_NumOftopics[c_index][original_topic]-=1\n",
    "    c_NumOftopics[c_index][z_d_token]+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_info=[]\n",
    "phi_c_info_array=[]\n",
    "phi_g_info=[]\n",
    "\n",
    "\n",
    "def generate_phi_theta():\n",
    "\t#theta_info=[]\n",
    "\tglobal theta_info\n",
    "\tglobal phi_c_info_array\n",
    "\tglobal phi_g_phi\n",
    "\t#theta_info=[]\n",
    "\t#phi_g_info=[]\n",
    "\tfor i in range(K):\n",
    "\t\tphi_g_info.append({})\n",
    "\t\n",
    "\t#phi_c_info_array=[]\n",
    "\tphi_c_info_array.append([])\n",
    "\tphi_c_info_array.append([])\n",
    "\tfor i in range(K):\n",
    "\t\tphi_c_info_array[0].append({})\n",
    "\t\tphi_c_info_array[1].append({})\n",
    "\n",
    "\tfor i in range(len(doc_topics)):\n",
    "\t\ttheta_info.append([])\n",
    "\t\tfor j in range(K):\n",
    "\t\t\ttemp=(doc_topics[i][j]+alpha)/(len(array[i])+K*alpha)\n",
    "\t\t\ttheta_info[len(theta_info)-1].append(temp)\n",
    "\t\n",
    "\tfor i in range(K):\n",
    "\t\tsum_nk=global_NumOftopics[i] \n",
    "\t\tsum_nk_c1=c_NumOftopics[1][i]\n",
    "\t\tsum_nk_c0=c_NumOftopics[0][i]\n",
    "\n",
    "\t\tfor ele in vocabulary:\n",
    "\t\t\tif ele in global_phi[i]:\n",
    "\t\t\t\tphi_g_info[i][ele]=(global_phi[i][ele]+beta)/(sum_nk+V*beta)\n",
    "\t\t\telse:\n",
    "\t\t\t\tphi_g_info[i][ele]=beta/(sum_nk+V*beta)\n",
    "\t\t\t\n",
    "\t\t\tif ele in collection_phi[0][i]:\n",
    "\t\t\t\tphi_c_info_array[0][i][ele]=(collection_phi[0][i][ele]+beta)/(sum_nk_c0+V*beta)\n",
    "\t\t\telse:\n",
    "\t\t\t\tphi_c_info_array[0][i][ele]=beta/(sum_nk_c0+V*beta)\n",
    "\t\t\t\n",
    "\t\t\tif ele in collection_phi[1][i]:\n",
    "\t\t\t\tphi_c_info_array[1][i][ele]=(collection_phi[1][i][ele]+beta)/(sum_nk_c1+V*beta)\n",
    "\t\t\telse:\n",
    "\t\t\t\tphi_c_info_array[1][i][ele]=beta/(sum_nk_c1+V*beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0\n",
      "23.34278678894043\n",
      "begin compute phi and theta\n",
      "iteration: 1\n"
     ]
    }
   ],
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
