{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of vocabulary is : 8096\n",
      "12164\n",
      "12177\n",
      "12294\n",
      "12241\n",
      "12213\n",
      "11898\n",
      "12248\n",
      "12194\n",
      "12171\n",
      "12079\n",
      "\n",
      "4487\n",
      "7677\n",
      "4369\n",
      "7808\n",
      "4478\n",
      "7816\n",
      "4420\n",
      "7821\n",
      "4382\n",
      "7831\n",
      "4281\n",
      "7617\n",
      "4382\n",
      "7866\n",
      "4396\n",
      "7798\n",
      "4346\n",
      "7825\n",
      "4307\n",
      "7772\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from operator import itemgetter\n",
    "from random import randint\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "alpha=0.1\n",
    "beta=0.01\n",
    "lmd=0.5\n",
    "num_of_iterations=10\n",
    "burnin=8\n",
    "outputfile=\"output.txt\"\n",
    "inputfile_test=\"input-test.txt\"\n",
    "inputfile_train=\"input-train.txt\"\n",
    "K=10#the number of topics\n",
    "\n",
    "## create global phi\n",
    "global_phi=[]\n",
    "for i in range(K):\n",
    "\tglobal_phi.append({})\n",
    "\n",
    "\n",
    "## create collection phi\n",
    "collection_phi=[]\n",
    "collection_phi.append([])\n",
    "collection_phi.append([])\n",
    "\n",
    "\n",
    "#collection_0_ph=[]\n",
    "for i in range(K):\n",
    "\tcollection_phi[0].append({})\n",
    "\tcollection_phi[1].append({})\n",
    "\n",
    "doc_topics=[]\n",
    "vocabulary=set()\n",
    "V=0\n",
    "\n",
    "array=[]\n",
    "x=[]\n",
    "global_NumOftopics=[]\n",
    "for i in range(K):\n",
    "\tglobal_NumOftopics.append(0)\n",
    "\n",
    "c_NumOftopics=[]\n",
    "c_NumOftopics.append([])\n",
    "c_NumOftopics.append([])\n",
    "for i in range(K):\n",
    "\tc_NumOftopics[0].append(0)\n",
    "\tc_NumOftopics[1].append(0)\n",
    "\n",
    "\n",
    "\n",
    "with open(inputfile_train,'r') as f:\n",
    "    for line in f:\n",
    "        tokens=line.strip(\"\\n\").split(\" \")\n",
    "        array.append([])\n",
    "        doc_topics.append([])\n",
    "        x.append([])\n",
    "        size_of_doc=len(doc_topics)\n",
    "        \n",
    "        for i in range(K):\n",
    "            doc_topics[size_of_doc-1].append(0)\n",
    "\n",
    "        for token in tokens:\n",
    "            vocabulary.add(token)\n",
    "            random_topic=randint(0,K-1)\n",
    "            x_d_i=np.random.choice(2,1,p=[1-lmd,lmd])[0]\n",
    "            x[len(x)-1].append(x_d_i)\n",
    "            array[len(array)-1].append(random_topic)\n",
    "            global_NumOftopics[random_topic]+=1\n",
    "            # construct docs\n",
    "            doc_topics[size_of_doc-1][random_topic]+=1\n",
    "            if token in global_phi[random_topic]:\n",
    "                global_phi[random_topic][token]+=1\n",
    "            else:\n",
    "                global_phi[random_topic][token]=1\n",
    "            if tokens[0]=='1':\n",
    "            \tc_NumOftopics[1][random_topic]+=1\n",
    "            \tif token in collection_phi[1][random_topic]:\n",
    "            \t\tcollection_phi[1][random_topic][token]+=1\n",
    "            \telse:\n",
    "            \t\tcollection_phi[1][random_topic][token]=1\n",
    "            elif tokens[0]=='0':\n",
    "            \tc_NumOftopics[0][random_topic]+=1\n",
    "            \tif token in collection_phi[0][random_topic]:\n",
    "            \t\tcollection_phi[0][random_topic][token]+=1\n",
    "            \telse:\n",
    "            \t\tcollection_phi[0][random_topic][token]=1\n",
    "    f.close()\n",
    "\n",
    "#print(type(vocabulary))\n",
    "V=len(vocabulary)\n",
    "print(\"the number of vocabulary is :\",V)\n",
    "for i in range(K):\n",
    "\tprint(global_NumOftopics[i])\n",
    "print()\n",
    "for i in range(K):\n",
    "\tprint(c_NumOftopics[0][i])\n",
    "\tprint(c_NumOftopics[1][i])\n",
    "\n",
    "\n",
    "\n",
    "x_test=[]\n",
    "array_test=[]\n",
    "doc_topics_test=[]\n",
    "with open(\"input-test.txt\",\"r\") as f:\n",
    "    d=-1\n",
    "    for line in f:\n",
    "        d+=1\n",
    "        tokens=line.strip(\"\\n\").split(\" \")\n",
    "        array_test.append([])\n",
    "        x_test.append([])\n",
    "        doc_topics_test.append([])\n",
    "        for i in range(K):\n",
    "            doc_topics_test[len(doc_topics_test)-1].append(0)\n",
    "        for i in range(len(tokens)):\n",
    "            random_topic=randint(0,K-1)\n",
    "            x_d_i=np.random.choice(2,1,p=[1-lmd,lmd])[0]\n",
    "            doc_topics_test[len(doc_topics_test)-1][random_topic]+=1\n",
    "            x_test[len(x_test)-1].append(x_d_i)\n",
    "            array_test[d].append(random_topic)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "##store data\n",
    "data=None\n",
    "with open(inputfile_train,'r') as f:\n",
    "\tdata=f.readlines()\n",
    "\tf.close()\n",
    "train_data=[]\n",
    "for line in data:\n",
    "\ttokens=line.strip(\"\\n\").split(\" \")\n",
    "\ttrain_data.append(data)\n",
    "\n",
    "\n",
    "test_data1=None\n",
    "with open(inputfile_test,'r') as f:\n",
    "\ttest_data1=f.readlines()\n",
    "\tf.close()\n",
    "test_data=[]\n",
    "for line in test_data1:\n",
    "\ttokens=line.strip(\"\\n\").split(\" \")\n",
    "\ttest_data.append(tokens)\n",
    "\n",
    "#print(test_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_distribution_doc_topic(flag,c_index,doc_index,token,doc_topics):\n",
    "    global K\n",
    "    global alpha\n",
    "    global beta\n",
    "    global vocabulary\n",
    "    global global_phi\n",
    "    global collection_phi\n",
    "    \n",
    "    distributions=[]\n",
    "    if flag==1:\n",
    "        for i in range(K):\n",
    "            distribution_i=(doc_topics[doc_index][i]+alpha)/(len(array[doc_index])-1+K*alpha)\n",
    "            sum_nk=c_NumOftopics[c_index][i]\n",
    "            # for ele in vocabulary:\n",
    "            #     if ele in collection_phi[c_index][i]:\n",
    "            #         sum_nk+=collection_phi[c_index][i][ele]\n",
    "            if token in collection_phi[c_index][i]:\n",
    "            \tdistribution_i*=(collection_phi[c_index][i][token]+beta)/(sum_nk+V*beta)\n",
    "            else:\n",
    "            \tdistribution_i*=(beta)/(sum_nk+V*beta)\n",
    "            distributions.append(distribution_i)\n",
    "    elif flag==0:\n",
    "        for i in range(K):\n",
    "            distribution_i=(doc_topics[doc_index][i]+alpha)/(len(array[doc_index])-1+K*alpha)\n",
    "            sum_nk=global_NumOftopics[i]\n",
    "            # for ele in vocabulary:\n",
    "            #     if ele in global_phi[i]:\n",
    "            #         sum_nk+=global_phi[i][ele]\n",
    "            if token in global_phi[i]:\n",
    "            \tdistribution_i*=(global_phi[i][token]+beta)/(sum_nk+V*beta)\n",
    "            else:\n",
    "            \tdistribution_i*=beta/(sum_nk+V*beta)\n",
    "            distributions.append(distribution_i)\n",
    "    \n",
    "    return np.array(distributions)/np.sum(distributions)\n",
    "\n",
    "def smaple_xdi(c_index,doc_index,token,x,z_d_token):\n",
    "    global vocabulary\n",
    "    global global_phi\n",
    "    global collection_phi\n",
    "    global lmd\n",
    "    global V\n",
    "    \n",
    "    sum_nk=global_NumOftopics[z_d_token]\n",
    "    # for ele in global_phi[z_d_token]:\n",
    "    #     sum_nk+=global_phi[z_d_token][ele]\n",
    "    p_0=0\n",
    "    if token in global_phi[z_d_token]:\n",
    "    \tp_0=(1-lmd)*((global_phi[z_d_token][token]+beta)/(sum_nk+V*beta))\n",
    "    else:\n",
    "    \tp_0=(1-lmd)*(beta/(sum_nk+V*beta))\n",
    "\n",
    "    sum_nk_c=c_NumOftopics[c_index][z_d_token]\n",
    "    # for ele in collection_phi[c_index][z_d_token]:\n",
    "    #     sum_nk_c+=collection_phi[c_index][z_d_token][ele]\n",
    "    p_1=0\n",
    "    if token in collection_phi[c_index][z_d_token]:\n",
    "    \tp_1=lmd*((collection_phi[c_index][z_d_token][token]+beta)/(sum_nk_c+V*beta))\n",
    "    else:\n",
    "    \tp_1=lmd*(beta/(sum_nk_c+V*beta))\n",
    "    \n",
    "    p=[]\n",
    "    p.append(p_0)\n",
    "    p.append(p_1)\n",
    "    dis=np.array(p)/np.sum(p)\n",
    "    return np.random.choice(2,1,p=dis)[0]\n",
    "\n",
    "\n",
    "def update_dic(z_d_token,token,token_index,doc_index,c_index,flag):\n",
    "    global vocabulary\n",
    "    global global_phi\n",
    "    global collection_phi\n",
    "    global doc_topics\n",
    "    global array\n",
    "    \n",
    "    original_topic=array[doc_index][token_index]\n",
    "    #print(\"g:\",global_phi[original_topic][token])\n",
    "    global_phi[original_topic][token]-=1\n",
    "    #print(\"g:\",global_phi[original_topic][token])\n",
    "    if token in global_phi[z_d_token]:\n",
    "    \tglobal_phi[z_d_token][token]+=1\n",
    "    else:\n",
    "    \tglobal_phi[z_d_token][token]=1\n",
    "    collection_phi[c_index][original_topic][token]-=1\n",
    "    #print(\"c:\",collection_phi[c_index][original_topic][token])\n",
    "    if token in collection_phi[c_index][z_d_token]:\n",
    "    \tcollection_phi[c_index][z_d_token][token]+=1\n",
    "    else:\n",
    "    \tcollection_phi[c_index][z_d_token][token]=1\n",
    "    array[doc_index][token_index]=z_d_token\n",
    "    doc_topics[doc_index][original_topic]-=1\n",
    "    doc_topics[doc_index][z_d_token]+=1\n",
    "    \n",
    "    global_NumOftopics[original_topic]-=1\n",
    "    global_NumOftopics[z_d_token]+=1\n",
    "    c_NumOftopics[c_index][original_topic]-=1\n",
    "    c_NumOftopics[c_index][z_d_token]+=1  \n",
    "\n",
    "\n",
    "\n",
    "theta_info=[]\n",
    "phi_g_info=[]\n",
    "for i in range(K):\n",
    "\tphi_g_info.append({})\n",
    "\n",
    "phi_c_info_array=[]\n",
    "phi_c_info_array.append([])\n",
    "phi_c_info_array.append([])\n",
    "for i in range(K):\n",
    "\tphi_c_info_array[0].append({})\n",
    "\tphi_c_info_array[1].append({})\n",
    "\n",
    "def generate_phi_theta():\n",
    "\t#theta_info=[]\n",
    "\tfor i in range(len(doc_topics)):\n",
    "\t\ttheta_info.append([])\n",
    "\t\tfor j in range(K):\n",
    "\t\t\ttemp=(doc_topics[i][j]+alpha)/(len(array[i])+K*alpha)\n",
    "\t\t\ttheta_info[len(theta_info)-1].append(temp)\n",
    "\t\n",
    "\tfor i in range(K):\n",
    "\t\tsum_nk=0\n",
    "\t\tfor ele in vocabulary:\n",
    "\t\t\tif ele in global_phi[i]:\n",
    "\t\t\t\tsum_nk+=global_phi[i][ele] \n",
    "\t\tsum_nk_c1=0\n",
    "\t\tfor ele in vocabulary:\n",
    "\t\t\tif ele in collection_phi[1][i]:\n",
    "\t\t\t\tsum_nk_c1+=collection_phi[1][i][ele]\n",
    "\t\tsum_nk_c0=0\n",
    "\t\tfor ele in vocabulary:\n",
    "\t\t\tif ele in collection_phi[0][i]:\n",
    "\t\t\t\tsum_nk_c0+=collection_phi[0][i][ele]\n",
    "\n",
    "\t\tfor ele in vocabulary:\n",
    "\t\t\tif ele in global_phi[i]:\n",
    "\t\t\t\tphi_g_info[i][ele]=(global_phi[i][ele]+beta)/(sum_nk+V*beta)\n",
    "\t\t\telse:\n",
    "\t\t\t\tphi_g_info[i][ele]=beta/(sum_nk+V*beta)\n",
    "\t\t\t\n",
    "\t\t\tif ele in collection_phi[0][i]:\n",
    "\t\t\t\tphi_c_info_array[0][i][ele]=(collection_phi[0][i][ele]+beta)/(sum_nk_c0+V*beta)\n",
    "\t\t\telse:\n",
    "\t\t\t\tphi_c_info_array[0][i][ele]=beta/(sum_nk_c0+V*beta)\n",
    "\t\t\t\n",
    "\t\t\tif ele in collection_phi[1][i]:\n",
    "\t\t\t\tphi_c_info_array[1][i][ele]=(collection_phi[1][i][ele]+beta)/(sum_nk_c1+V*beta)\n",
    "\t\t\telse:\n",
    "\t\t\t\tphi_c_info_array[1][i][ele]=beta/(sum_nk_c1+V*beta)\n",
    "\n",
    "\n",
    "def output_tofile(output_tofile):\n",
    "\tlines=output_tofile.split(\"-\")\n",
    "\twith open(output_tofile,\"a\") as f:\n",
    "\t\tlist_vocabulary=list(vocabulary)\n",
    "\t\tif lines[1]==\"phi\":\n",
    "\t\t\tfor ele in vocabulary:\n",
    "\t\t\t\ttempstr=ele\n",
    "\t\t\t\tfor i in range(K):\n",
    "\t\t\t\t\ttempstr+=\" {:.13f}\".format(store_c_phi[i][ele]/(num_of_iterations-burnin))\n",
    "\t\t\t\tf.write(tempstr)\n",
    "\t\t\t\tf.write(\"\\n\")\n",
    "\t\telif lines[1]==\"phi0\":\n",
    "\t\t\tfor ele in vocabulary:\n",
    "\t\t\t\ttempstr=ele\n",
    "\t\t\t\tfor i in range(K):\n",
    "\t\t\t\t\ttempstr+=\" {:.13f}\".format(store_c_phi[0][ele]/(num_of_iterations-burnin))\n",
    "\t\t\t\tf.write(tempstr)\n",
    "\t\t\t\tf.write(\"\\n\")\n",
    "\t\telif lines[1]=='phi1':\n",
    "\t\t\tfor ele in vocabulary:\n",
    "\t\t\t\ttempstr=ele\n",
    "\t\t\t\tfor i in range(K):\n",
    "\t\t\t\t\ttempstr+=\" {:.13f}\".format(store_c_phi[1][ele]/(num_of_iterations-burnin))\n",
    "\t\telif lines[1]=='theta':\n",
    "\t\t\tfor i in range(len(store_theta)):\n",
    "\t\t\t\ttempstr=\"document1 \"\n",
    "\t\t\t\tfor j in range(K):\n",
    "\t\t\t\t\ttempstr+=\" {:.13f}\".format(store_phi[i][j]/(num_of_iterations-burnin))\n",
    "\t\t\t\tf.write(tempstr)\n",
    "\t\t\t\tf.write(\"\\n\")\n",
    "\t\telif lines[1]==\"trainll\":\n",
    "\t\t\tpass\n",
    "\t\telif lines[1]==\"testll\":\n",
    "\t\t\tpass\n",
    "\n",
    "\n",
    "\n",
    "store_theta=None\n",
    "#store_test_theta=np.zeros(len(test_theta_info),len(test_theta_info[0]))\n",
    "store_g_phi=[]\n",
    "for i in range(K):\n",
    "\tstore_g_phi.append({})\n",
    "store_c_phi=[]\n",
    "store_c_phi.append([])\n",
    "store_c_phi.append([])\n",
    "\n",
    "for i in range(K):\n",
    "\tstore_c_phi[0].append({})\n",
    "\tstore_c_phi[1].append({})\n",
    "\n",
    "def burn_in():\n",
    "\tglobal store_theta\n",
    "\tif store_theta==None:\n",
    "\t\tstore_theta=np.zeros(len(theta_info),len(theta_info[0]))\n",
    "\n",
    "\tfor i in range(len(theta_info)):\n",
    "\t\tfor j in range(len(theta_info[0])):\n",
    "\t\t\tstore_phi[i][j]+=theta_info[i][j]\n",
    "\n",
    "\t# for i in range(len(test_theta_info)):\n",
    "\t# \tfor j in range(len(test_theta_info[0])):\n",
    "\t# \t\tstore_test_theta[i][j]=test_theta_info[i][j]\n",
    "\n",
    "\tfor i in range(K):\n",
    "\t\tfor ele in vocabulary:\n",
    "\t\t\tif ele in store_g_phi[i]:\n",
    "\t\t\t\tstore_g_phi[i][ele]+=phi_g_info[i][ele]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstore_g_phi[i][ele]=phi_g_info[i][ele]\n",
    "\t\t\tif ele in store_c_phi[0][i]:\n",
    "\t\t\t\tstore_c_phi[0][i][ele]+=phi_c_info_array[0][i][ele]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstore_c_phi[0][i][ele]=phi_c_info_array[0][i][ele]\n",
    "\n",
    "\t\t\tif ele in store_c_phi[1][i]:\n",
    "\t\t\t\tstore_c_phi[1][i][ele]+=phi_c_info_array[1][i][ele]\n",
    "\t\t\telse:\n",
    "\t\t\t\tstore_c_phi[1][ele]=phi_c_info_array[1][i][ele]\n",
    "\n",
    "\n",
    "\n",
    "def compute_z_x_testingfile():\n",
    "    with open(inputfile_test,\"r\") as f:\n",
    "    \td=-1\n",
    "    \tfor line in f:\n",
    "    \t\td+=1\n",
    "    \t\ttokens=line.strip(\"\\n\").split(\" \")\n",
    "    \t\ti=-1\n",
    "    \t\tfor token in tokkens:\n",
    "    \t\t\ti+=1\n",
    "    \t\t\tflag=c[d][i]\n",
    "    \t\t\tp_distributions=get_distribution_doc_topic(flag,int(tokens[0]),d,token,doc_topics_test)\n",
    "    \t\t\tz_d_token=np.random.choice(K,1,p=p_distributions)[0]\n",
    "    \t\t\tx_d_i=smaple_xdi(int(tokens[0]),d,token,x_test,z_d_token)\n",
    "    \t\t\tupdate_test_file_info(x_d_i,d,i,z_d_token)\n",
    "    \tf.close()    \t\t\t\n",
    "\n",
    "\n",
    "def update_test_file_info(x_d_i,doc_index,token_index,z_d_token):\n",
    "\tglobal array_test\n",
    "\toriginaltopic=array_test[doc_index][token_index]\n",
    "\tdoc_topics_test[doc_index][originaltopic]-=1\n",
    "\tdoc_topics_test[doc_topics][z_d_token]+=1\n",
    "\tarray[doc_index][token_index]=z_d_token\n",
    "\tx_test[doc_index][token_index]=x_d_i\n",
    "\n",
    "\n",
    "test_theta_info=[]\n",
    "def compute_test_phi_theta():\n",
    "\tfor i in range(len(doc_topics_test)):\n",
    "\t\ttest_theta_info.append([])\n",
    "\t\tfor j in range(K):\n",
    "\t\t\ttemp=(doc_topics_test[i][j]+alpha)/(len(array_test)+K*alpha)\n",
    "\t\t\ttest_theta_info[len(test_theta_info)-1].append(temp)\n",
    "\n",
    "\n",
    "def compute_test_log_likelihood():\n",
    "\tpass\n",
    "\n",
    "def compute_train_log_likelihood(doc_topics,array):\n",
    "\ttotal_likelihiid=0\n",
    "\tglbal_topic=[]\n",
    "\tc_topic=[]\n",
    "\tc_topic.append([])\n",
    "\tc_topic.append([])\n",
    "\tfor i in range(K):\n",
    "\t\ttemp_sum=0\n",
    "\t\ttemp_sum_c0=0\n",
    "\t\ttemp_sum_c1=0\n",
    "\t\tfor ele in vocabulary:\n",
    "\t\t\tif ele in global_phi[i]:\n",
    "\t\t\t\ttemp_sum+=global_phi[i][ele]\n",
    "\t\t\tif ele in collection_phi[0][i]:\n",
    "\t\t\t\ttemp_sum_c0+=collection_phi[0][i][ele]\n",
    "\t\t\tif ele in collection_phi[1][i]:\n",
    "\t\t\t\ttemp_sum_c1+=collection_phi[1][i][ele]\n",
    "\t\tglobal_topic.append(temp_sum)\n",
    "\t\tc_topic[0].append(temp_sum_c0)\n",
    "\t\tc_topic[1].append(temp_sum_c1)\n",
    "\twith open(sys.argv[1],'r') as f:\n",
    "\t\td=-1\n",
    "\t\tfor line in f:\n",
    "\t\t\td+=1\n",
    "\t\t\ttokens=line.strip(\"\\n\").split(\" \")\n",
    "\t\t\ttokens_sum=0\n",
    "\t\t\tfor token in tokens:\n",
    "\t\t\t\tpre_log_sum=0\n",
    "\t\t\t\tfor k in range(K):\n",
    "\t\t\t\t\ttheta_dz=(doc_topics[d][k]+alpha)/(len(array[d])+K*alpha)\n",
    "\t\t\t\t\tphi_z_wdi=(global_phi[k][token]+beta)/(global_topic[k]+V*beta)\n",
    "\t\t\t\t\tphi_z_wdi_cd=0\n",
    "\t\t\t\t\tif int(tokens[0])==1:\n",
    "\t\t\t\t\t\tphi_z_wdi_cd=(collection_phi[1][k][token]+beta)/(c_topic[1][k]+V*beta)\n",
    "\t\t\t\t\telif int(tokens[0])==0:\n",
    "\t\t\t\t\t\tphi_z_wdi_cd=(collection_phi[0][k][token]+beta)/(c_topic[0][k]+V*beta)\n",
    "\t\t\t\t\tpre_log_sum+=theta_dz*((1-lmd)*phi_z_wdi+lmd*phi_z_wdi_cd)\n",
    "\t\t\t\ttokens_sum+=np.log(pre_log_sum)\n",
    "\t\t\ttotal_likelihiid+=tokens_sum\n",
    "\t\tf.close()\n",
    "\treturn total_likelihiid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '1 interface node abstract source filter annotator user defined java class implements loaded chain visualization represented separate box handles details related drawing various visual cues display gr",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c196d4236cf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mz_d_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_distribution_doc_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_d_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '1 interface node abstract source filter annotator user defined java class implements loaded chain visualization represented separate box handles details related drawing various visual cues display gr"
     ]
    }
   ],
   "source": [
    "for t in range(num_of_iterations):\n",
    "    d=-1\n",
    "    start=time.time()\n",
    "    for tokens in train_data:\n",
    "        d+=1\n",
    "        i=-1\n",
    "        for token in tokens:\n",
    "            i+=1\n",
    "            flag=x[d][i]\n",
    "            print(flag,int(tokens[0]),d,token)\n",
    "            z_d_token=np.random.choice(K,1,p=get_distribution_doc_topic(flag,int(tokens[0]),d,token,doc_topics))[0]\n",
    "            print(z_d_token)\n",
    "            x[d][i]=smaple_xdi(int(tokens[0]),d,token,x,z_d_token)\n",
    "            #print(x[d][i])\n",
    "            update_dic(z_d_token,token,i,d,int(tokens[0]),flag)\n",
    "    end=time.time()\n",
    "    print(end-start)\n",
    "    print(\"begin compute phi and theta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data=[]\n",
    "for line in data:\n",
    "    train_data.append(line.strip(\"\\n\").split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'interface',\n",
       " 'node',\n",
       " 'abstract',\n",
       " 'source',\n",
       " 'filter',\n",
       " 'annotator',\n",
       " 'user',\n",
       " 'defined',\n",
       " 'java',\n",
       " 'class',\n",
       " 'implements',\n",
       " 'loaded',\n",
       " 'chain',\n",
       " 'visualization',\n",
       " 'represented',\n",
       " 'separate',\n",
       " 'box',\n",
       " 'handles',\n",
       " 'details',\n",
       " 'related',\n",
       " 'drawing',\n",
       " 'various',\n",
       " 'visual',\n",
       " 'cues',\n",
       " 'display',\n",
       " 'graphical',\n",
       " 'implemented',\n",
       " 'set',\n",
       " 'components',\n",
       " 'right',\n",
       " 'displays',\n",
       " 'current',\n",
       " 'described',\n",
       " 'previous',\n",
       " 'section',\n",
       " 'allow',\n",
       " 'create',\n",
       " 'modify',\n",
       " 'tune',\n",
       " 'new',\n",
       " 'chains',\n",
       " 'built',\n",
       " 'pre',\n",
       " 'existing',\n",
       " 'figure',\n",
       " 'macro',\n",
       " 'running',\n",
       " 'provides',\n",
       " 'types',\n",
       " 'feedback',\n",
       " 'regarding',\n",
       " 'task',\n",
       " 'progress',\n",
       " 'indicate',\n",
       " 'percentage',\n",
       " 'overall',\n",
       " 'run',\n",
       " 'time',\n",
       " 'active',\n",
       " 'border',\n",
       " 'color',\n",
       " 'varies',\n",
       " 'green',\n",
       " 'red',\n",
       " 'output',\n",
       " 'unit',\n",
       " 'spent',\n",
       " 'indicates',\n",
       " 'bytes',\n",
       " 'second',\n",
       " 'text',\n",
       " 'label',\n",
       " 'meter',\n",
       " 'graphic',\n",
       " 'relative',\n",
       " 'throughput',\n",
       " 'highest',\n",
       " 'solid',\n",
       " 'nodes',\n",
       " 'level',\n",
       " 'shows',\n",
       " 'maximum',\n",
       " 'library',\n",
       " 'tree',\n",
       " 'view',\n",
       " 'upper',\n",
       " 'left',\n",
       " 'currently',\n",
       " 'available',\n",
       " 'machine',\n",
       " 'building',\n",
       " 'extending',\n",
       " 'directories',\n",
       " 'downloaded',\n",
       " 'web',\n",
       " 'added',\n",
       " 'component',\n",
       " 'examines',\n",
       " 'using',\n",
       " 'reflection',\n",
       " 'capabilities',\n",
       " 'places']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
